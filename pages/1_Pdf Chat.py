from dotenv import load_dotenv
import streamlit as st
import os
from streamlit_extras.row import row 

def gen_chatgpt_response():
   pass

def clear_chat_history():
  st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]
  
def generate_chatgpt_response():
   pass

# ËÆæÁΩÆÊ†è
# with st.sidebar:
#     add_title = st.write(
#         "Settings",
#     )
#     if 'OPENAI_API_KEY' in st.secrets:
#         st.success('API key already provided!', icon='‚úÖ')
#         openai_api_key = st.secrets['OPENAI_API_KEY']
#     else:
#         openai_api_key = st.text_input('Enter openai API token:', type='password')
#         if not len(openai_api_key)==40:
#             st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')
#         else:
#             st.success('Proceed to entering your prompt message!', icon='üëâ')
#             os.environ['OPENAI_API_KEY'] = openai_api_key
#     with st.container():
#         row_settings_model = row(1, vertical_align="top")
#         row_settings_model.selectbox("Select model", ["gpt3.5", "gpt4"])
#         row_settings_t = row(1, vertical_align="top")
#         temperature = row_settings_t.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)
#         row_settings_save = row(1, vertical_align="top")
#         row_settings_save.button("Save", use_container_width=True)
#         st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# # Store LLM generated responses
# if "messages" not in st.session_state.keys():
#     st.session_state.messages = [{"role": "assistant", "content": "How may I assist you today?"}]

# # Display or clear chat messages
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.write(message["content"])


# # User-provided prompt
# if prompt := st.chat_input(disabled=not openai_api_key):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.write(prompt)

# # Generate a new response if last message is not from assistant
# if st.session_state.messages[-1]["role"] != "assistant":
#     with st.chat_message("assistant"):
#         with st.spinner("Thinking..."):
#             response = generate_chatgpt_response(prompt)
#             placeholder = st.empty()
#             full_response = ''
#             for item in response:
#                 full_response += item
#                 placeholder.markdown(full_response)
#             placeholder.markdown(full_response)
#     message = {"role": "assistant", "content": full_response}
#     st.session_state.messages.append(message)

#


import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain.memory import ConversationBufferMemory
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.text_splitter import RecursiveCharacterTextSplitter

st.set_page_config(page_title="Chat with PDF", page_icon="üìÑ")
st.header('Chat with your pdf documents')
st.divider()


class CustomDataChatbot:

    def __init__(self):
        utils.configure_openai_api_key()
        self.openai_model = "gpt-4"

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        
        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)

        # Create embeddings and store in vectordb
        embeddings = OpenAIEmbeddings()
        vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True
        )

        # Setup LLM and QA chain
        llm = ChatOpenAI(model_name=self.openai_model, temperature=0, streaming=True)
        qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory, verbose=True)
        return qa_chain

    @utils.enable_chat_history
    def main(self):

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='Upload PDF files', type=['pdf'], accept_multiple_files=True)
        if not uploaded_files:
            st.error("Please upload PDF documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                response = qa_chain.run(user_query, callbacks=[st_cb])
                st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    obj = CustomDataChatbot()
    obj.main()